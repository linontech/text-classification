{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### author  : lzw\n",
    "### description: using gensim doc2vec to handle sentimentclassification ( doc2vec as text representation )\n",
    "### reference : tutorial on gensim official\n",
    "       (https://github.com/RaRe-Technologies/gensim/blob/develop/docs/notebooks/doc2vec-IMDB.ipynb)\n",
    "### opinion: \n",
    "  - Use an external metric to evaluate doc2vec embedding.(sentiment classification) \n",
    "  \n",
    "   - Imbalanced dataset of sentiment classification, might a bad metric to evaluate doc2vec.\n",
    "  \n",
    "  - It might be useful to throw away stopwords for a small dataset.\n",
    "  \n",
    "### requirement\n",
    "  - python35"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\William\\Anaconda3\\envs\\python35\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n",
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\William\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 1.211 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "import multiprocessing\n",
    "from collections import OrderedDict\n",
    "from random import shuffle\n",
    "import gensim\n",
    "import jieba\n",
    "import numpy as np\n",
    "from gensim.models.doc2vec import Doc2Vec\n",
    "from sklearn import linear_model\n",
    "from sklearn.metrics import classification_report\n",
    "from data_helper import load_text_label, elapsed_timer, split_file_classes\n",
    "import sys ; sys.path.insert(0,\"../\")\n",
    "import utils\n",
    "\n",
    "jieba.load_userdict('../dataset/digital forum comments/digital_selfdict.txt')\n",
    "cores = multiprocessing.cpu_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train texts num: 74346\n",
      "dev texts num: 31864\n",
      "unk texts num 245907\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "    load corpus and specific split portions.\n",
    "\"\"\"\n",
    "corpus_path = '../dataset/digital forum comments/split_20180403/classes/'\n",
    "label_index, class_name_count = split_file_classes('../dataset/digital forum comments/split_20180403/all.txt')\n",
    "split = [0.7, 0.3]\n",
    "corpus, train_len_total, dev_len_total = load_text_label(corpus_path, class_name_count, label_index, split, shuffle=True)\n",
    "corpus_train = [d for d in corpus if d.split == 'train']\n",
    "corpus_dev = [d for d in corpus if d.split == 'dev']\n",
    "corpus_unk = [d for d in corpus if d.split == 'UNK']# sentences with unknown label\n",
    "\n",
    "corpus_len = len(corpus)\n",
    "print ('train texts num: %d' %(train_len_total))\n",
    "print ('dev texts num: %d' %(dev_len_total))\n",
    "print ('unk texts num %d' %(len(corpus_unk)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doc2Vec(dm/c,d100,n20,w5,mc3,s0.001,t4)\n",
      "Doc2Vec(dbow,d100,n20,mc3,s0.001,t4)\n",
      "Doc2Vec(dm/m,d100,n20,w5,mc3,s0.001,t4)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "    specific doc2vec model parameters\n",
    "\"\"\"\n",
    "simple_models = [\n",
    "    # PV-DM w/ concatenation - window=5 (both sides) approximates paper's 10-word total window size\n",
    "    Doc2Vec(dm=1, dm_concat=1, vector_size=100, window=5, negative=20, hs=0, min_count=3, workers=cores),\n",
    "    # PV-DBOW\n",
    "    Doc2Vec(dm=0, vector_size=100, negative=20, hs=0, min_count=3, workers=cores),\n",
    "    # PV-DM w/ average\n",
    "    Doc2Vec(dm=1, dm_mean=1, vector_size=100, window=5, negative=20, hs=0, min_count=3, workers=cores),\n",
    "]\n",
    "models_by_name = OrderedDict((str(model), model) for model in simple_models)\n",
    "\n",
    "# throw stop words away\n",
    "stop_words_file = '../dataset/stopwords.txt'\n",
    "stop_words_file = open(stop_words_file, encoding='utf-8')\n",
    "stop_words = set(stop_words_file.read().split('\\n'))\n",
    "stop_words_file.close()\n",
    "def my_trim(word, count, min_count):\n",
    "    if word in stop_words:\n",
    "        return gensim.utils.RULE_DISCARD\n",
    "    else:\n",
    "        return gensim.utils.RULE_DEFAULT\n",
    "    \n",
    "simple_models[0].build_vocab(corpus, trim_rule=my_trim)\n",
    "# simple_models[0].build_vocab_from_freq(word2count)# PV-DM w/ concat requires one special NULL word so it serves as template\n",
    "print(simple_models[0])\n",
    "for model in simple_models[1:]:\n",
    "    model.reset_from(simple_models[0])\n",
    "    print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "START 2018-04-05 21:36:42.325323\n",
      "[complete epoch 1 ]\n",
      "[complete epoch 2 ]\n",
      "[complete epoch 3 ]\n",
      "[complete epoch 4 ]\n",
      "[complete epoch 5 ]\n",
      "[complete epoch 6 ]\n",
      "[complete epoch 7 ]\n",
      "[complete epoch 8 ]\n",
      "[complete epoch 9 ]\n",
      "[complete epoch 10 ]\n",
      "[complete epoch 11 ]\n",
      "[complete epoch 12 ]\n",
      "[complete epoch 13 ]\n",
      "[complete epoch 14 ]\n",
      "[complete epoch 15 ]\n",
      "[complete epoch 16 ]\n",
      "[complete epoch 17 ]\n",
      "[complete epoch 18 ]\n",
      "[complete epoch 19 ]\n",
      "[complete epoch 20 ]\n",
      "END 2018-04-05 22:04:23.550819\n"
     ]
    }
   ],
   "source": [
    "alpha, min_alpha, epoches = 0.01, 0.025, 30\n",
    "alpha_delta = (alpha - min_alpha) / epoches\n",
    "print(\"START %s\" % datetime.datetime.now())\n",
    "for epoch in range(1, epoches + 1):\n",
    "    shuffle(corpus)\n",
    "    for name, train_model in models_by_name.items():\n",
    "        train_model.alpha, train_model.min_alpha = alpha, alpha\n",
    "        train_model.train(corpus, total_examples=corpus_len, epochs=1)  # Train\n",
    "    alpha -= alpha_delta\n",
    "    print('[complete epoch %d ]' % (epoch))\n",
    "\n",
    "print(\"END %s\" % str(datetime.datetime.now()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## word vectors similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_models = simple_models[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "most similar words for '蓝屏' (304 occurences)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\William\\Anaconda3\\envs\\python35\\lib\\site-packages\\ipykernel\\__main__.py:12: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table><tr><th>Doc2Vec(dm/c,d100,n20,w5,mc3,s0.001,t4)</th><th>Doc2Vec(dbow,d100,n20,mc3,s0.001,t4)</th><th>Doc2Vec(dm/m,d100,n20,w5,mc3,s0.001,t4)</th></tr><tr><td>[('死机', 0.6453672647476196),<br>\n",
       "('掉帧', 0.5336572527885437),<br>\n",
       "('黑屏', 0.5276408195495605),<br>\n",
       "('不好受', 0.5066460967063904),<br>\n",
       "('重启', 0.5064347982406616),<br>\n",
       "('卡住', 0.49836763739585876),<br>\n",
       "('啪', 0.4920652210712433),<br>\n",
       "('卡死', 0.48987439274787903),<br>\n",
       "('扫射', 0.4890672564506531),<br>\n",
       "('driverpower', 0.4881700277328491),<br>\n",
       "('浓', 0.4871233105659485),<br>\n",
       "('消失', 0.4867590665817261),<br>\n",
       "('花屏', 0.48554563522338867),<br>\n",
       "('闭幕', 0.48437780141830444),<br>\n",
       "('脱胎换骨', 0.4842427372932434),<br>\n",
       "('闪退', 0.48232191801071167),<br>\n",
       "('失火', 0.48080918192863464),<br>\n",
       "('开不了机', 0.478066623210907),<br>\n",
       "('抖动', 0.4774223268032074),<br>\n",
       "('贡酒', 0.47672390937805176)]</td><td>[('炮火', 0.3934079706668854),<br>\n",
       "('山东地区', 0.38848552107810974),<br>\n",
       "('告别', 0.3857855796813965),<br>\n",
       "('主动出击', 0.3735121488571167),<br>\n",
       "('实质', 0.37349337339401245),<br>\n",
       "('Pay', 0.37048235535621643),<br>\n",
       "('言论', 0.3557182252407074),<br>\n",
       "('B型', 0.353630393743515),<br>\n",
       "('长沙地区', 0.35167157649993896),<br>\n",
       "('主城', 0.3503320813179016),<br>\n",
       "('摔', 0.3464968800544739),<br>\n",
       "('GPS', 0.34526538848876953),<br>\n",
       "('九千', 0.34388646483421326),<br>\n",
       "('白板', 0.34240207076072693),<br>\n",
       "('潍坊', 0.33825549483299255),<br>\n",
       "('备', 0.33548614382743835),<br>\n",
       "('回溯', 0.3300383687019348),<br>\n",
       "('logo', 0.3257746696472168),<br>\n",
       "('肺', 0.32479560375213623),<br>\n",
       "('秀才', 0.324740469455719)]</td><td>[('死机', 0.5976219773292542),<br>\n",
       "('重装系统', 0.5648543238639832),<br>\n",
       "('FPS', 0.5384677648544312),<br>\n",
       "('卡死', 0.5288370251655579),<br>\n",
       "('开不开机', 0.504610002040863),<br>\n",
       "('闪退', 0.5013353824615479),<br>\n",
       "('重启', 0.5004960894584656),<br>\n",
       "('开机', 0.4987723231315613),<br>\n",
       "('花屏', 0.4918825924396515),<br>\n",
       "('win', 0.49185311794281006),<br>\n",
       "('莫名其妙', 0.48777681589126587),<br>\n",
       "('进不了', 0.47796082496643066),<br>\n",
       "('自动', 0.4706798195838928),<br>\n",
       "('卡住', 0.46173620223999023),<br>\n",
       "('重装', 0.4606199264526367),<br>\n",
       "('黑屏', 0.4562179446220398),<br>\n",
       "('光盘', 0.4546934962272644),<br>\n",
       "('卡机', 0.45238950848579407),<br>\n",
       "('lol', 0.44898971915245056),<br>\n",
       "('报错', 0.4460287094116211)]</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "from IPython.display import HTML\n",
    "# pick a random word with a suitable number of occurences\n",
    "# while True:\n",
    "#     word = random.choice(word_models[0].wv.index2word)\n",
    "#     if word_models[0].wv.vocab[word].count > 2000:\n",
    "#         break\n",
    "word='蓝屏'\n",
    "# word='华为'\n",
    "# or uncomment below line, to just pick a word from the relevant domain:\n",
    "#word = 'comedy/drama'\n",
    "similars_per_model = [str(model.most_similar(word, topn=20)).replace('), ','),<br>\\n') for model in word_models]\n",
    "similar_table = (\"<table><tr><th>\" +\n",
    "    \"</th><th>\".join([str(model) for model in word_models]) + \n",
    "    \"</th></tr><tr><td>\" +\n",
    "    \"</td><td>\".join(similars_per_model) +\n",
    "    \"</td></tr></table>\")\n",
    "print(\"most similar words for '%s' (%d occurences)\" % (word, simple_models[0].wv.vocab[word].count))\n",
    "HTML(similar_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## doc vectors similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TARGET (331550): « 联想拯救者 R 可以 装 固态 吗 »\n",
      "\n",
      "SIMILAR/DISSIMILAR DOCS PER MODEL Doc2Vec(dm/c,d100,n20,w5,mc3,s0.001,t4):\n",
      "\n",
      "MOST (211464, 0.8575758934020996): « 长虹 华为 签署 战略 合作 协议 共推 智慧 城市 发展 »\n",
      "\n",
      "MEDIAN (263460, 0.009524062275886536): « 华为 最 经典 的 外观设计 为什么 没有 延续 使用 下去 »\n",
      "\n",
      "LEAST (65436, -0.7224101424217224): « 任正非 计划 未来 几年 华为 每年 研发 经费 要 提高 到 亿美元 »\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "doc_id = np.random.randint(simple_models[0].docvecs.count)  # pick random doc, re-run cell for more examples\n",
    "model = random.choice(simple_models)  # and a random model\n",
    "sims = model.docvecs.most_similar(doc_id, topn=model.docvecs.count)  # get *all* similar documents\n",
    "print(u'TARGET (%d): « %s »\\n' % (doc_id, ' '.join(corpus[doc_id].words)))\n",
    "print(u'SIMILAR/DISSIMILAR DOCS PER MODEL %s:\\n' % model)\n",
    "for label, index in [('MOST', 0), ('MEDIAN', len(sims)//2), ('LEAST', len(sims) - 1)]:\n",
    "    print(u'%s %s: « %s »\\n' % (label, sims[index], ' '.join(corpus[sims[index][0]].words)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## sentiment classificatioin\n",
    "### 0：负面； 1：中立"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "infer_steps = 20\n",
    "infer_alpha = 0.01\n",
    "infer_min_alpha=0.025"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- model: Doc2Vec(dm/c,d100,n20,w5,mc3,s0.001,t4)   ----------\n",
      "-- Epoch 1\n",
      "Norm: 2.81, NNZs: 99, Bias: -0.110767, T: 74346, Avg. loss: 0.703230\n",
      "Total training time: 0.07 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 3.78, NNZs: 94, Bias: -0.223693, T: 148692, Avg. loss: 0.699638\n",
      "Total training time: 0.17 seconds.\n",
      "Convergence after 2 epochs took 0.17 seconds\n",
      " [clf  Doc2Vec(dm/c,d100,n20,w5,mc3,s0.001,t4) ]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.21      0.89      0.34      6620\n",
      "          1       0.79      0.11      0.20     25244\n",
      "\n",
      "avg / total       0.67      0.27      0.22     31864\n",
      "\n",
      " [clf  Doc2Vec(dm/c,d100,n20,w5,mc3,s0.001,t4) infer 20 ]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.21      0.80      0.33      6620\n",
      "          1       0.79      0.20      0.32     25244\n",
      "\n",
      "avg / total       0.67      0.32      0.32     31864\n",
      "\n",
      "------------------------------\n",
      "---------- model: Doc2Vec(dbow,d100,n20,mc3,s0.001,t4)   ----------\n",
      "-- Epoch 1\n",
      "Norm: 8.76, NNZs: 100, Bias: 0.325056, T: 74346, Avg. loss: 0.454470\n",
      "Total training time: 0.09 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 9.37, NNZs: 99, Bias: 0.257848, T: 148692, Avg. loss: 0.438552\n",
      "Total training time: 0.15 seconds.\n",
      "Convergence after 2 epochs took 0.15 seconds\n",
      " [clf  Doc2Vec(dbow,d100,n20,mc3,s0.001,t4) ]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.43      0.89      0.58      6620\n",
      "          1       0.96      0.69      0.80     25244\n",
      "\n",
      "avg / total       0.85      0.73      0.75     31864\n",
      "\n",
      " [clf  Doc2Vec(dbow,d100,n20,mc3,s0.001,t4) infer 20 ]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.44      0.88      0.59      6620\n",
      "          1       0.96      0.71      0.81     25244\n",
      "\n",
      "avg / total       0.85      0.74      0.77     31864\n",
      "\n",
      "------------------------------\n",
      "---------- model: Doc2Vec(dm/m,d100,n20,w5,mc3,s0.001,t4)   ----------\n",
      "-- Epoch 1\n",
      "Norm: 9.58, NNZs: 98, Bias: 0.462477, T: 74346, Avg. loss: 0.570445\n",
      "Total training time: 0.06 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 10.49, NNZs: 96, Bias: 0.446938, T: 148692, Avg. loss: 0.550124\n",
      "Total training time: 0.12 seconds.\n",
      "Convergence after 2 epochs took 0.12 seconds\n",
      " [clf  Doc2Vec(dm/m,d100,n20,w5,mc3,s0.001,t4) ]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.39      0.76      0.52      6620\n",
      "          1       0.92      0.69      0.79     25244\n",
      "\n",
      "avg / total       0.81      0.70      0.73     31864\n",
      "\n",
      " [clf  Doc2Vec(dm/m,d100,n20,w5,mc3,s0.001,t4) infer 20 ]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.32      0.81      0.45      6620\n",
      "          1       0.92      0.54      0.68     25244\n",
      "\n",
      "avg / total       0.79      0.60      0.63     31864\n",
      "\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "sentiment_clf_models = {}\n",
    "\n",
    "for name, train_model in models_by_name.items():\n",
    "    \n",
    "    print ('-'*10 + ' model: ' + name + '   ' + '-'*10)\n",
    "    dev_labels = [doc.label for doc in corpus if doc.split == 'dev']\n",
    "    clf = linear_model.SGDClassifier(max_iter=1000, verbose=1, tol=10, learning_rate='constant', eta0=0.01,\n",
    "                                     class_weight={1: 1, 0: 4}, loss='log', penalty='elasticnet')\n",
    "\n",
    "    clf.fit([train_model.docvecs[doc.tags[0]] for doc in corpus if doc.split == 'train'],\n",
    "            [doc.label for doc in corpus if doc.split == 'train'])\n",
    "\n",
    "    classifedLabels_dev = clf.predict(\n",
    "        [train_model.docvecs[doc.tags[0]] for doc in corpus if doc.split == 'dev'])\n",
    "    print(\n",
    "        ' [clf  ' + name + ' ]' + '\\n' + str(classification_report(dev_labels, classifedLabels_dev)))\n",
    "\n",
    "    classifedLabels_dev_infer = clf.predict(np.array( # infer dev texts to evaluate doc2vec model \n",
    "        [train_model.infer_vector(doc.words, steps=infer_steps, alpha=infer_alpha, min_alpha=infer_min_alpha) for doc in corpus if\n",
    "         doc.split == 'dev']))\n",
    "    print(\n",
    "        ' [clf  ' + name + ' infer ' + str(infer_steps) +  ' ]' + '\\n' + str(\n",
    "            classification_report(dev_labels, classifedLabels_dev_infer)) + '\\n')\n",
    "    \n",
    "    sentiment_clf_models[name] = clf\n",
    "    print ('-'*70)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test texts "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### using unseen texts to evaluate sentiment classification logistic regression "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# for unseen text, what is a good infer parameters？\n",
    "# small learning rate might only inferring noises.\n",
    "test_infer_steps = 30\n",
    "test_infer_alpha = 0.05\n",
    "test_infer_min_alpha=0.025"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_filepath = '../dataset/digital forum comments/测试集3_联想_杨欣标注15000_20171116.txt'\n",
    "test_texts, test_labels = utils.load_data_label(test_filepath)\n",
    "test_labels = [1-l for l in test_labels]\n",
    "negative_len = len([l for l in test_labels if l==0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len of negative: 1251 , len of positive: 13737, portion:1 : 0.091068 \n",
      "\n",
      "---------- model: Doc2Vec(dbow,d100,n20,mc3,s0.001,t4)   ----------\n",
      " [clf  Doc2Vec(dbow,d100,n20,mc3,s0.001,t4) infer 30 ]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.11      0.35      0.16      1251\n",
      "          1       0.93      0.74      0.82     13737\n",
      "\n",
      "avg / total       0.86      0.71      0.77     14988\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "---------- model: Doc2Vec(dm/c,d100,n20,w5,mc3,s0.001,t4)   ----------\n",
      " [clf  Doc2Vec(dm/c,d100,n20,w5,mc3,s0.001,t4) infer 30 ]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.08      0.73      0.15      1251\n",
      "          1       0.91      0.24      0.38     13737\n",
      "\n",
      "avg / total       0.84      0.28      0.36     14988\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "---------- model: Doc2Vec(dm/m,d100,n20,w5,mc3,s0.001,t4)   ----------\n",
      " [clf  Doc2Vec(dm/m,d100,n20,w5,mc3,s0.001,t4) infer 30 ]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.14      0.83      0.24      1251\n",
      "          1       0.97      0.53      0.69     13737\n",
      "\n",
      "avg / total       0.90      0.56      0.65     14988\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print ('len of negative: %d , len of positive: %d, portion: %d : %f \\n' % (negative_len, len(test_labels)-negative_len, \n",
    "                                                                       1,negative_len/(len(test_labels)-negative_len)))\n",
    "\n",
    "for name, clf_model in sentiment_clf_models.items():\n",
    "    \n",
    "    print ('-'*10 + ' model: ' + name + '   ' + '-'*10)\n",
    "    classifedLabels_test_infer = clf_model.predict(np.array( # infer dev texts to evaluate doc2vec model \n",
    "        [models_by_name[name].infer_vector(jieba.lcut(''.join(utils.accept_sentence(doc)), HMM=False), \n",
    "                                           steps=test_infer_steps, alpha=test_infer_steps, min_alpha=test_infer_alpha) \n",
    "                                           for doc in test_texts]))\n",
    "    \n",
    "    print(\n",
    "        ' [clf  ' + name + ' infer ' + str(infer_steps) +  ' ]' + '\\n' + str(\n",
    "            classification_report(test_labels, classifedLabels_test_infer)) + '\\n')\n",
    "    \n",
    "    print ('-'*70)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:python35]",
   "language": "python",
   "name": "conda-env-python35-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
